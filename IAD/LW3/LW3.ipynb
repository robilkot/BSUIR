{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Задача 3. Применение рекуррентной нейронной сети для аннотации текста\n",
    "1. Загрузить текстовый корпус, например, набор предложений с метками части речи (POS-tagging).\n",
    "2. Построить и обучить рекуррентную нейронную сеть (RNN или LSTM) для предсказания аннотаций текста.\n",
    "3. Оценить модель на тестовом наборе и сравнить предсказанные метки с реальными.\n",
    "4. Вывести 5 примеров предложений с предсказанными и реальными аннотациями.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e319b9e0c014e25b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Загрузка корпуса с метками части речи\n",
    "\n",
    "Загрузим корпус в виде итератора пар (слово - часть речи). Игнорируем морфологические признаки из датасета.\n",
    "Каждое предложение завершаем точкой, если нету никакого знака препинания. Это пригодится для моделей, использующих контекст токенов."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce34b92931678904"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "f = open(\"samples3\", mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "def process_line(line: str) -> list[str]:\n",
    "    tokens = line.split('\\t')\n",
    "    return tokens\n",
    "    \n",
    "def iter_process(it):\n",
    "    punkt = ['.', 'PUNCT']\n",
    "    \n",
    "    last_pos = 'X'\n",
    "    try:\n",
    "        while True:\n",
    "            line = next(it)\n",
    "            tokens = process_line(line)\n",
    "            if len(tokens) < 2:\n",
    "                continue    \n",
    "            elif tokens[1][0] == '<':\n",
    "                if tokens[1][1] == 'e' and last_pos != 'PUNCT':  #end\n",
    "                    last_pos = 'PUNCT'\n",
    "                    yield punkt\n",
    "                continue\n",
    "            \n",
    "            word_str = tokens[1]\n",
    "            characteristics_str = tokens[2]\n",
    "            \n",
    "            sep_index = characteristics_str.find('|')\n",
    "            pos_str = characteristics_str[:sep_index]\n",
    "            \n",
    "            last_pos = pos_str\n",
    "            yield [word_str, pos_str]\n",
    "    except StopIteration:\n",
    "        it.seek(0)\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:03:22.560795800Z",
     "start_time": "2025-03-24T11:03:22.548951100Z"
    }
   },
   "id": "a93b1389e415c733",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['рой', 'VERB']\n",
      "['колодец', 'NOUN']\n",
      "['.', 'PUNCT']\n",
      "['рой', 'VERB']\n",
      "['погреб', 'NOUN']\n",
      "['.', 'PUNCT']\n",
      "['рой', 'VERB']\n",
      "['укрытие', 'NOUN']\n",
      "['.', 'PUNCT']\n",
      "['я', 'PRON']\n"
     ]
    }
   ],
   "source": [
    "corpus_it = iter_process(f)\n",
    "corpus = list(corpus_it)\n",
    "\n",
    "for i in range(10):\n",
    "    print(corpus[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:03:41.270087100Z",
     "start_time": "2025-03-24T11:03:39.378590300Z"
    }
   },
   "id": "dcaee7e2f5dd1aab",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Попытка обучить нейросеть с прямой связью\n",
    "Первый вопрос, который возник при виде условия - \"Почему LSTM/RNN?\"\n",
    "В качестве эксперимента и ответа на вопрос рассмотрим результаты обучения нейросети только с Linear и Dropout слоями на самописных эмбеддингах слов.\n",
    "\n",
    "### Расчёт эмбеддингов слов. \n",
    "Рассчитываем эмбеддинг на основе количества символов с конца слова (предполагаем, что последние символы - изменяемые части слова, поэтому могут определять часть речи) для расчёта.\n",
    "\n",
    "### Предобработка результирующего столбца (часть речи)\n",
    "Категориальный признак преобразуем в числовое значение в диапазоне $[1, n]$, где $n$ - количество классов (частей речи) в датасете."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f79bb671c568ba8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [  0   0   0   0   0   0   0   0   0 224 222 217] 1\n",
      "1 [  0   0   0   0   0 218 222 219 222 212 213 230] 0\n",
      "2 [ 0  0  0  0  0  0  0  0  0  0  0 46] 5\n",
      "3 [  0   0   0   0   0   0   0   0   0 224 222 217] 1\n",
      "4 [  0   0   0   0   0   0 223 222 211 224 213 209] 0\n"
     ]
    }
   ],
   "source": [
    "from wannabe_embedder import make_embeddings_and_outputs\n",
    "\n",
    "n = 12\n",
    "embedding_dim = n\n",
    "\n",
    "indexes_in_corpus, embeddings, outputs = make_embeddings_and_outputs(corpus, n)\n",
    "\n",
    "for i in range(5):\n",
    "    print(indexes_in_corpus[i], embeddings[i], outputs[i])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:43:17.460388300Z",
     "start_time": "2025-03-24T10:43:14.104734700Z"
    }
   },
   "id": "95fb5b457a5eb154",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from helpers import save_embeddings_data\n",
    "\n",
    "save_embeddings_data(indexes_in_corpus, embeddings, outputs, 'embeddings/embeddings.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a7100b4beab0eb2",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from helpers import load_embeddings_data\n",
    "\n",
    "indexes_in_corpus, embeddings, outputs = load_embeddings_data('embeddings/embeddings.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eb5ebf4db066a75",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Построение и обучение нейросети"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "774e875b2f1dc25f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# !nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:43:23.564086200Z",
     "start_time": "2025-03-24T10:43:21.204458700Z"
    }
   },
   "id": "8bc32b6b880a8c04",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Преобразуем массивы входных данных в тензоры PyTorch, создадим загрузчик данных. Размер батча задаём большим, так как используется GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de7d6d8898b7eac0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tensor_embeddings = torch.Tensor(embeddings).to(device)\n",
    "tensor_outputs = torch.Tensor(outputs).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:43:27.826258600Z",
     "start_time": "2025-03-24T10:43:27.622265Z"
    }
   },
   "id": "a501f6d669882054",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pos_dataset = TensorDataset(tensor_embeddings, tensor_outputs)\n",
    "batch_size = 2048\n",
    "pos_dataloader = DataLoader(pos_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:43:29.685892500Z",
     "start_time": "2025-03-24T10:43:29.675822Z"
    }
   },
   "id": "94a12d8fd84a9747",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Зададим нейросеть с прямыми связями"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8281e5b88e25a778"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimplePOSTagger(\n",
      "  (fc1): Linear(in_features=12, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (fc3): Linear(in_features=128, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from constants import POS_MAPPING\n",
    "\n",
    "\n",
    "class SimplePOSTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, num_tags):\n",
    "        super(SimplePOSTagger, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim).to(device)\n",
    "        # self.fc2 = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "        self.dropout = nn.Dropout(0.1).to(device)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_tags).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "hidden_dim = 128\n",
    "num_tags = len(POS_MAPPING)\n",
    "model = SimplePOSTagger(embedding_dim=embedding_dim, hidden_dim=hidden_dim, num_tags=num_tags).to(device)\n",
    "print(model)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:43:37.340863200Z",
     "start_time": "2025-03-24T10:43:35.891175900Z"
    }
   },
   "id": "463e95ec55429b57",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,\tloss: 3.6374,\taccuracy: 0.4261\n",
      "epoch 2,\tloss: 1.1185,\taccuracy: 0.5610\n",
      "epoch 3,\tloss: 1.0566,\taccuracy: 0.5734\n",
      "epoch 4,\tloss: 1.0375,\taccuracy: 0.5808\n",
      "epoch 5,\tloss: 1.0236,\taccuracy: 0.5864\n",
      "epoch 6,\tloss: 1.0137,\taccuracy: 0.5895\n",
      "epoch 7,\tloss: 1.0042,\taccuracy: 0.5939\n",
      "epoch 8,\tloss: 0.9964,\taccuracy: 0.5970\n",
      "epoch 9,\tloss: 0.9888,\taccuracy: 0.6001\n",
      "epoch 10,\tloss: 0.9782,\taccuracy: 0.6060\n",
      "epoch 11,\tloss: 0.9689,\taccuracy: 0.6105\n",
      "epoch 12,\tloss: 0.9603,\taccuracy: 0.6140\n",
      "epoch 13,\tloss: 0.9535,\taccuracy: 0.6157\n",
      "epoch 14,\tloss: 0.9470,\taccuracy: 0.6184\n",
      "epoch 15,\tloss: 0.9442,\taccuracy: 0.6189\n",
      "epoch 16,\tloss: 0.9391,\taccuracy: 0.6210\n",
      "epoch 17,\tloss: 0.9351,\taccuracy: 0.6219\n",
      "epoch 18,\tloss: 0.9309,\taccuracy: 0.6238\n",
      "epoch 19,\tloss: 0.9270,\taccuracy: 0.6253\n",
      "epoch 20,\tloss: 0.9241,\taccuracy: 0.6267\n",
      "epoch 21,\tloss: 0.9208,\taccuracy: 0.6282\n",
      "epoch 22,\tloss: 0.9173,\taccuracy: 0.6292\n",
      "epoch 23,\tloss: 0.9146,\taccuracy: 0.6298\n",
      "epoch 24,\tloss: 0.9134,\taccuracy: 0.6302\n",
      "epoch 25,\tloss: 0.9108,\taccuracy: 0.6312\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(pos_dataloader):\n",
    "        # Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss_value = loss(outputs, labels.long())\n",
    "        \n",
    "        # Backward pass и оптимизация\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss_value.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(pos_dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    print(f'epoch {epoch + 1},\\tloss: {epoch_loss:.4f},\\taccuracy: {accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:47:56.496342Z",
     "start_time": "2025-03-24T10:43:41.083635700Z"
    }
   },
   "id": "99178acbc8eacf7",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from constants import MODEL_PATH\n",
    "from helpers import save_model\n",
    "\n",
    "save_model(model, optimizer, num_epochs, epoch_loss, MODEL_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:48:08.109884900Z",
     "start_time": "2025-03-24T10:48:08.071341Z"
    }
   },
   "id": "dff62bfa3af79233",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Примеры предсказаний"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bedffa76b2f7dce1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            labels.extend(targets.cpu().numpy())\n",
    "            \n",
    "    return predictions, labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:45:14.076644400Z",
     "start_time": "2025-03-24T11:45:14.067056400Z"
    }
   },
   "id": "ebcdc131ec0aec07",
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Слово         |     Прогноз     |    Фактически  \n",
      "--------------------------------------------------------\n",
      "ребята               |      NOUN       |      NOUN      \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "которые              |      NOUN       |       ADJ      \n",
      "посмелей             |      NOUN       |       ADJ      \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "конечно              |      NOUN       |       ADV      \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "уехали               |      NOUN       |      VERB      \n",
      "на                   |       ADP       |       ADP      \n",
      "крышах               |      NOUN       |      NOUN      \n",
      "и                    |       ADP       |      CONJ      \n",
      "на                   |       ADP       |       ADP      \n",
      "подножках            |      VERB       |      NOUN      \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "а                    |       ADP       |      CONJ      \n",
      "я                    |       ADP       |      PRON      \n",
      "не                   |      PART       |      PART      \n",
      "могла                |      NOUN       |      VERB      \n",
      "так                  |       ADV       |       ADV      \n",
      "…                    |      PUNCT      |      PUNCT     \n"
     ]
    }
   ],
   "source": [
    "from constants import INVERSE_POS_MAPPING\n",
    "\n",
    "test_f = open(\"test_samples\", mode=\"r\", encoding=\"utf-8\")\n",
    "test_corpus_it = iter_process(test_f)\n",
    "test_corpus = list(test_corpus_it)\n",
    "\n",
    "test_f.close()\n",
    "\n",
    "test_idx_in_corpus, test_embeddings, test_outputs = make_embeddings_and_outputs(test_corpus, n)\n",
    "\n",
    "test_embeddings = torch.Tensor(test_embeddings).to(device)\n",
    "test_outputs = torch.Tensor(test_outputs).to(device)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_embeddings,\n",
    "    test_outputs\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "predictions, labels = evaluate_model(model, test_dataloader)\n",
    "\n",
    "heads = ['Слово', 'Прогноз', 'Фактически']\n",
    "print(*[f\"{heads[0]:^20} | \", f\"{heads[1]:^14} | \", f\"{heads[2]:^14}\"])\n",
    "print(\"-\" * (22 + 17 + 17))\n",
    "\n",
    "for i, index_in_corpus in enumerate(test_idx_in_corpus):\n",
    "    word = test_corpus[index_in_corpus][0]\n",
    "    pred_tag = INVERSE_POS_MAPPING[predictions[i]]\n",
    "    actual_tag = test_corpus[index_in_corpus][1]\n",
    "\n",
    "    print(f\"{word:20} | {pred_tag:^15} | {actual_tag:^15}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:50:17.887002100Z",
     "start_time": "2025-03-24T10:50:17.880237900Z"
    }
   },
   "id": "399adcbf0ff5f562",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from helpers import load_model\n",
    "\n",
    "loaded_model, loaded_optimizer, loaded_epoch, loaded_loss = load_model(model, optimizer, MODEL_PATH)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T10:48:57.420067700Z",
     "start_time": "2025-03-24T10:48:57.401425Z"
    }
   },
   "id": "71c7e1e85fd2cbb1",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Словарь с индексами слов для эмбеддингов"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0184f528a148f4a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SecondPOSTagger(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, \n",
    "                 pos_tag_count: int):\n",
    "        super(SecondPOSTagger, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc2 = nn.Linear(hidden_dim, pos_tag_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        out = F.relu(self.fc1(embeds))\n",
    "        out = self.dropout(out)\n",
    "        return self.fc2(out)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:13:00.119067Z",
     "start_time": "2025-03-24T11:13:00.094369100Z"
    }
   },
   "id": "eafa02b28740e37f",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "\n",
    "\n",
    "def prepare_data(corpus: List[Tuple[str, str]]) -> Tuple[Dict[str, int], \n",
    "                                                        Dict[str, int]]:\n",
    "    word_to_ix = {}\n",
    "    pos_to_ix = {}\n",
    "    \n",
    "    # Создаем словари для слов и частей речи\n",
    "    for word, pos in corpus:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "        if pos not in pos_to_ix:\n",
    "            pos_to_ix[pos] = len(pos_to_ix)\n",
    "    \n",
    "    return word_to_ix, pos_to_ix\n",
    "\n",
    "def train_model(model: SecondPOSTagger, device: torch.device, \n",
    "                optimizer: torch.optim.Optimizer, loss_fn: nn.CrossEntropyLoss,\n",
    "                batch: Tuple[torch.Tensor, torch.Tensor]) -> float:\n",
    "    \"\"\"Обучение модели на одном батче\"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    inputs, labels = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate_second_model(model: SecondPOSTagger, device: torch.device, loss_fn: nn.CrossEntropyLoss, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    inputs, labels = batch\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    accuracy = (predicted == labels).sum().item() / len(labels)\n",
    "    \n",
    "    return loss.item(), accuracy\n",
    "\n",
    "\n",
    "def apply_model_on_new_data(model: SecondPOSTagger, device: torch.device, word_to_ix: Dict[str, int], pos_to_ix: Dict[str, int]) -> None:\n",
    "    test_f = open(\"test_samples\", mode=\"r\", encoding=\"utf-8\")\n",
    "    test_corpus_it = iter_process(test_f)\n",
    "    test_corpus = list(test_corpus_it)\n",
    "    test_f.close()\n",
    "    \n",
    "    word_indices = [word_to_ix.get(word, 0) for word, _ in test_corpus]\n",
    "    pos_indices = [pos_to_ix[pos] for _, pos in test_corpus]\n",
    "    \n",
    "    inputs_tensor = torch.tensor(word_indices, dtype=torch.long).to(device)\n",
    "    labels_tensor = torch.tensor(pos_indices, dtype=torch.long).to(device)\n",
    "    \n",
    "    test_dataset = TensorDataset(inputs_tensor, labels_tensor)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    predictions, labels = evaluate_model(model, test_dataloader)\n",
    "    \n",
    "    heads = ['Слово', 'Прогноз', 'Фактически']\n",
    "    print(*[f\"{heads[0]:^20} | \", f\"{heads[1]:^14} | \", f\"{heads[2]:^14}\"])\n",
    "    print(\"-\" * (22 + 17 + 17))\n",
    "    \n",
    "    # Получаем обратные отображения для частей речи\n",
    "    inverse_pos_mapping = {v: k for k, v in pos_to_ix.items()}\n",
    "    \n",
    "    for i, (word, pos) in enumerate(test_corpus):\n",
    "        pred_tag = inverse_pos_mapping[predictions[i]]\n",
    "        actual_tag = pos\n",
    "        print(f\"{word:20} | {pred_tag:^15} | {actual_tag:^15}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:44:40.690132400Z",
     "start_time": "2025-03-24T11:44:40.676980600Z"
    }
   },
   "id": "4aa05175f993943b",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SecondPOSTagger(\n",
      "  (embedding): Embedding(111008, 100)\n",
      "  (fc1): Linear(in_features=100, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=12, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "batch_size = 1024\n",
    "epochs = 10\n",
    "\n",
    "word_to_ix, pos_to_ix = prepare_data(corpus)\n",
    "vocab_size = len(word_to_ix)\n",
    "pos_tag_count = len(pos_to_ix)\n",
    "\n",
    "# Создание модели и оптимизатора\n",
    "model = SecondPOSTagger(vocab_size, embedding_dim, hidden_dim, pos_tag_count)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:51:36.248555900Z",
     "start_time": "2025-03-24T11:51:35.914293900Z"
    }
   },
   "id": "b50d9adced0d6a0c",
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 500/1020, train loss: 0.6738, test loss: 0.5755, accuracy: 0.7200\n",
      "epoch 1, batch 1000/1020, train loss: 0.5755, test loss: 0.4469, accuracy: 0.8300\n",
      "epoch 2, batch 500/1020, train loss: 0.3655, test loss: 0.3688, accuracy: 0.8600\n",
      "epoch 2, batch 1000/1020, train loss: 0.3396, test loss: 0.3480, accuracy: 0.8900\n",
      "epoch 3, batch 500/1020, train loss: 0.2526, test loss: 0.3195, accuracy: 0.9100\n",
      "epoch 3, batch 1000/1020, train loss: 0.2407, test loss: 0.3041, accuracy: 0.9100\n",
      "epoch 4, batch 500/1020, train loss: 0.1888, test loss: 0.2997, accuracy: 0.9100\n",
      "epoch 4, batch 1000/1020, train loss: 0.1824, test loss: 0.2753, accuracy: 0.9200\n",
      "epoch 5, batch 500/1020, train loss: 0.1457, test loss: 0.2780, accuracy: 0.9100\n",
      "epoch 5, batch 1000/1020, train loss: 0.1431, test loss: 0.2929, accuracy: 0.9100\n",
      "epoch 6, batch 500/1020, train loss: 0.1169, test loss: 0.2697, accuracy: 0.9200\n",
      "epoch 6, batch 1000/1020, train loss: 0.1154, test loss: 0.2830, accuracy: 0.9200\n",
      "epoch 7, batch 500/1020, train loss: 0.0966, test loss: 0.2882, accuracy: 0.9300\n",
      "epoch 7, batch 1000/1020, train loss: 0.0953, test loss: 0.2635, accuracy: 0.9400\n",
      "epoch 8, batch 500/1020, train loss: 0.0807, test loss: 0.2799, accuracy: 0.9400\n",
      "epoch 8, batch 1000/1020, train loss: 0.0803, test loss: 0.3080, accuracy: 0.9300\n",
      "epoch 9, batch 500/1020, train loss: 0.0692, test loss: 0.2869, accuracy: 0.9300\n",
      "epoch 9, batch 1000/1020, train loss: 0.0690, test loss: 0.3016, accuracy: 0.9400\n",
      "epoch 10, batch 500/1020, train loss: 0.0602, test loss: 0.3285, accuracy: 0.9300\n",
      "epoch 10, batch 1000/1020, train loss: 0.0601, test loss: 0.3065, accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "word_indices = [word_to_ix[word] for word, _ in corpus]\n",
    "pos_indices = [pos_to_ix[pos] for _, pos in corpus]\n",
    "\n",
    "train_inputs = torch.tensor(word_indices[:-100], dtype=torch.long)\n",
    "train_labels = torch.tensor(pos_indices[:-100], dtype=torch.long)\n",
    "test_inputs = torch.tensor(word_indices[-100:], dtype=torch.long)\n",
    "test_labels = torch.tensor(pos_indices[-100:], dtype=torch.long)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    indices = torch.randperm(len(train_inputs))\n",
    "    train_inputs = train_inputs[indices]\n",
    "    train_labels = train_labels[indices]\n",
    "    \n",
    "    total_loss = 0\n",
    "    batches = len(train_inputs) // batch_size\n",
    "    \n",
    "    for i in range(batches):\n",
    "        start_idx = i * batch_size\n",
    "        inputs_batch = train_inputs[start_idx:start_idx + batch_size]\n",
    "        labels_batch = train_labels[start_idx:start_idx + batch_size]\n",
    "        \n",
    "        batch = (inputs_batch, labels_batch)\n",
    "        loss = train_model(model, device, optimizer, loss_fn, batch)\n",
    "        total_loss += loss\n",
    "        \n",
    "        if (i + 1) % 500 == 0:\n",
    "            test_batch = (test_inputs[:batch_size], test_labels[:batch_size])\n",
    "            test_loss, accuracy = evaluate_second_model(model, device, loss_fn, test_batch)\n",
    "            print(f'epoch {epoch+1}, batch {i+1}/{batches}, train loss: {total_loss/(i+1):.4f}, test loss: {test_loss:.4f}, accuracy: {accuracy:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:53:31.712663Z",
     "start_time": "2025-03-24T11:51:44.221434Z"
    }
   },
   "id": "d50d783952aeaf85",
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from helpers import save_model\n",
    "\n",
    "save_model(model, optimizer, num_epochs, epoch_loss, \"pos_tagger_model2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:55:50.345856600Z",
     "start_time": "2025-03-24T11:55:49.732341Z"
    }
   },
   "id": "1197bbf6c79419de",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Слово         |     Прогноз     |    Фактически  \n",
      "--------------------------------------------------------\n",
      "очень                |       ADV       |        X       \n",
      "сложная              |       ADJ       |        X       \n",
      "лабораторная         |      VERB       |        X       \n",
      "попалась             |      VERB       |        X       \n",
      ",                    |      PUNCT      |        X       \n",
      "не                   |      PART       |        X       \n",
      "получается           |      VERB       |        X       \n",
      "её                   |      VERB       |        X       \n",
      "доделать             |      VERB       |        X       \n",
      "в                    |       ADP       |        X       \n",
      "срок                 |      NOUN       |        X       \n",
      ".                    |      PUNCT      |      PUNCT     \n",
      "ребята               |      NOUN       |      NOUN      \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "которые              |       ADJ       |       ADJ      \n",
      "посмелей             |      VERB       |        X       \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "конечно              |       ADV       |       ADV      \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "уехали               |      VERB       |      VERB      \n",
      "на                   |       ADP       |       ADP      \n",
      "крышах               |      NOUN       |      NOUN      \n",
      "и                    |      CONJ       |      CONJ      \n",
      "на                   |       ADP       |       ADP      \n",
      "подножках            |      VERB       |      NOUN      \n",
      ",                    |      PUNCT      |      PUNCT     \n",
      "а                    |      CONJ       |      CONJ      \n",
      "я                    |      PRON       |      PRON      \n",
      "не                   |      PART       |      PART      \n",
      "могла                |      VERB       |      VERB      \n",
      "так                  |       ADV       |       ADV      \n",
      "…                    |      VERB       |      PUNCT     \n"
     ]
    }
   ],
   "source": [
    "apply_model_on_new_data(model, device, word_to_ix, pos_to_ix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-24T11:55:06.376801500Z",
     "start_time": "2025-03-24T11:55:06.364322Z"
    }
   },
   "id": "1aef7e3bfe7f4a06",
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "source": [
    "Получилась неплохая точность (95%), но модель не работает на данных, которых не было в маппинге тренировочного датасета. Это проблема эмбеддингов, а не самой модели.\n",
    "Также модель имеет очень большой размер (130 МБ) за счет слоя эмбеддингов. \n",
    "Попробуем устранить эти недостатки."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca93c60f5bc536f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Нормальные эмбеддинги (word2vec)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f208990f64e2d33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d9ee05efbac8c301"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
